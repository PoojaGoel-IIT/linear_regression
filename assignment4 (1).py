# -*- coding: utf-8 -*-
"""Assignment4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jWFzLvf4LR4yCmVjO9GWJUomv6vTgfDO

# metrics.py
"""

from typing import Union
import pandas as pd
import math
def accuracy(y_hat, y):
    assert(y_hat.size == y.size)
    # TODO: Write here
    c=0
    for i in range(y.size):
        if y_hat.iloc[i]==y.iloc[i]:
            c+=1
    return c/y.size

def precision(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]) -> float:
    """
    Function to calculate the precision
    """
    assert y_hat.size == y.size
    c=0
    for i in range(y.size):
        if y.iloc[i]==y_hat.iloc[i] and y.iloc[i]==cls:
            c+=1
    return c/y_hat.value_counts()[cls]
        

def recall(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]) -> float:
    """
    Function to calculate the recall
    """
    assert y_hat.size == y.size
    c=0
    for i in range(y.size):
        if y.iloc[i]==y_hat.iloc[i] and y.iloc[i]==cls:
            c+=1
    return c/y.value_counts()[cls]

def rmse(y_hat: pd.Series, y: pd.Series) -> float:
    """
    Function to calculate the root-mean-squared-error(rmse)
    """
    y_hat = pd.Series(y_hat)
    y = pd.Series(y)
    assert y_hat.size == y.size
    rms=0
    for i in range(y.size):
        rms+=math.pow(y_hat.iloc[i]-y.iloc[i],2)
    return math.sqrt(rms/y.size)

    

def mae(y_hat: pd.Series, y: pd.Series) -> float:
    """
    Function to calculate the mean-absolute-error(mae)
    """
    assert y_hat.size == y.size
    y_hat = pd.Series(y_hat)
    y = pd.Series(y)
    m_ae=0
    for i in range(y.size):
        m_ae+=abs(y_hat.iloc[i]-y.iloc[i])
    return m_ae/y.size

"""# linear_regression.py"""

# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import jax
import jax.numpy as jnp
from sklearn import linear_model 
from matplotlib import cm
np.random.seed(45)

class LinearRegression():
  def __init__(self, fit_intercept=True):
    # Initialize relevant variables
    '''
        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).
    '''
    self.fit_intercept =  fit_intercept 
    self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods
    # self.all_coef=pd.DataFrame([]) # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)
    self.all_coef = []

  def fit_sklearn_LR(self, X, y):
    # Solve the linear regression problem by calling Linear Regression
    # from sklearn, with the relevant parameters
    
    reg = linear_model.LinearRegression(fit_intercept=self.fit_intercept)
    reg.fit(X, y)
    self.coef_ = reg.coef_
    if self.fit_intercept:
      self.coef_ = np.insert(self.coef_, 0, reg.intercept_, axis =0)
    # reg.score(X, y)
    print(self.coef_)
    
  
  def fit_normal_equations(self, X, y):
    # Solve the linear regression problem using the closed form solution
    # to the normal equation for minimizing ||Wx - y||_2^2
    if self.fit_intercept==True:
      # X = np.hstack((np.ones((X.shape[0], 1)), X))
      ones = np.ones(len(X)).reshape(len(X),1)
      X = np.concatenate((ones, X), axis=1)
    
    self.X = np.array(X)
    self.y = np.array(y)
    
    XtX = X.T.dot(X)    
    XtX_inv = np.linalg.inv(XtX)
    Xty = X.T.dot(y)

    self.coef_ = XtX_inv.dot(Xty)
    # self.coef_ = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
    print(self.coef_)


  def fit_SVD(self, X, y):
    # Solve the linear regression problem using the SVD of the 
    # coefficient matrix

    # https://sthalles.github.io/svd-for-regression/
    if self.fit_intercept==True:
      # X = np.hstack((np.ones((X.shape[0], 1)), X))
      ones = np.ones(len(X)).reshape(len(X),1)
      X = np.concatenate((ones, X), axis=1)
    
    U,S,V = np.linalg.svd(X, full_matrices=False)
    S_inv = np.linalg.inv(np.diag(S))
    self.coef_ = V.T.dot(S_inv).dot(U.T).dot(y)
    print(self.coef_)

    
  def mse_loss(self, X, y):                
    # Compute the MSE loss with the learned model
    # y_hat = self.predict(X)
    y_hat = np.dot(X, self.coef_)
    return np.mean((y_hat - y)**2)
  

  def compute_gradient(self,X,y, penalty_type, penalty_value):
    # Compute the analytical gradient (in vectorized form) of the 
    # 1. unregularized mse_loss,  and 
    # 2. mse_loss with ridge regularization
    # penalty :  specifies the regularization used  , 'l2' or unregularized
    # gradient of unregularized mse loss

    
    y_hat = np.dot(X, self.coef_)
    y = y.reshape(X.shape[0],)
    
    n,d = X.shape
    if penalty_type == 'l2':
      # gradient of mse loss with ridge regularization
      gradient = (2/n) * X.T.dot(y_hat - y) + 2 *penalty_value* self.coef_
    else:
      # gradient of unregularized mse loss
      gradient = (2/n) * X.T.dot(y_hat - y)

    return gradient
  
  def compute_jax_gradient(self,X,y, penalty_type, penalty_value):
    # Compute the gradient of the 
    # 1. unregularized mse_loss, 
    # 2. mse_loss with LASSO regularization and 
    # 3. mse_loss with ridge regularization, using JAX 
    # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized

    X = np.array(X)
    y = np.array(y)
    y = y.reshape(X.shape[0],)
    
    def mse_loss_jax1(theta):
      y_hat = jnp.dot(X.astype('float'), theta)
      loss = jnp.mean((y_hat - y)**2)
      return loss    
    gradient_func = jax.grad(mse_loss_jax1, argnums=0)
    
    gradient = gradient_func(self.coef_)
    if penalty_type == 'l1':
      # gradient of mse loss with LASSO regularization
      gradient = gradient + penalty_value * np.sign(self.coef_)
    elif penalty_type == 'l2':
      # gradient of mse loss with ridge regularization
      gradient = gradient + 2 * penalty_value * self.coef_
    else:
      # gradient of unregularized mse loss
      gradient = gradient
    return gradient

  def fit_gradient_descent(self,X,y, batch_size, gradient_type, penalty_type,penalty_value=0, num_iters=20, lr=0.01):
    # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
    # batch_size : Number of training points in each batch
    # num_iters : Number of iterations of gradient descent
    # lr : Default learning rate
    # gradient_type : manual or JAX gradients
    # penalty_type : 'l1', 'l2' or unregularized

    # Create mini batches https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/
    if self.fit_intercept==True:
      # X = np.hstack((np.ones((X.shape[0], 1)), X))
      ones = np.ones(len(X)).reshape(len(X),1)
      X = np.concatenate((ones, X), axis=1)

    #-------------------------------------------------------------------------
    # batch code
    mini_batches = []
    y = np.array(y)
    y = y.reshape((-1,1))
    
    data = np.hstack((X, y))
    # np.random.shuffle(data) # shuffle the data
    n_minibatches = data.shape[0] // batch_size
    i = 0

    for i in range(n_minibatches):
        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]
        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))

    if data.shape[0] - (i+1)*batch_size:
        mini_batch = data[(i+1)*batch_size: , :]

        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))
    # --------------------------------------------------------------------------
    np.random.seed(45)
    self.coef_ = np.zeros(X.shape[1]) # initialize the coefficients
    for itr in range(num_iters):
      for mini_batch in mini_batches:
        X_mini, y_mini = mini_batch
        if gradient_type == 'manual':
          gradient = self.compute_gradient(X_mini,y_mini,penalty_type, penalty_value)
        else:
          gradient = self.compute_jax_gradient(X_mini,y_mini,penalty_type, penalty_value)
        self.coef_ = self.coef_ - lr * gradient
        self.all_coef.append(self.coef_)
    print('final coef ' ,self.coef_)

    return self.coef_

  def fit_SGD_with_momentum(self, X,y, num_iters=20, penalty_type='l2', penalty_value = 0, beta=0.9, lr=0.01):
    # Solve the linear regression problem using sklearn's implementation of SGD
    # penalty: refers to the type of regularization used (ridge)
    # beta: momentum parameter
     # https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/

    if self.fit_intercept==True:
      # X = np.hstack((np.ones((X.shape[0], 1)), X))
      ones = np.ones(len(X)).reshape(len(X),1)
      X = np.concatenate((ones, X), axis=1)
    #-------------------------------------------------------------------------
    # batch code
    mini_batches = []
    y = np.array(y)
    y = y.reshape((-1,1))
    
    data = np.hstack((X, y))
    # np.random.shuffle(data) # shuffle the data
    print('--------')
    batch_size = 1 # since SGD
    n_minibatches = data.shape[0] // batch_size
    i = 0

    for i in range(n_minibatches):
        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]
        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))

    if data.shape[0] - (i+1)*batch_size:
        mini_batch = data[(i+1)*batch_size: , :]

        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))
    # --------------------------------------------------------------------------

    self.coef_ = np.zeros(X.shape[1]) # initialize the coefficients
    velocity  = np.zeros(X.shape[1])
    for itr in range(num_iters):
      for mini_batch in mini_batches:
        X_mini, y_mini = mini_batch
        gradient = self.compute_gradient(X_mini,y_mini,penalty_type, penalty_value)
        # if penalty_type =='l2':
        #   gradient = gradient + 2 * penalty_value * self.coef_
        # velocity = beta * velocity + lr * gradient  
        # self.coef_ = self.coef_ - velocity   # update the coefficients

        # https://d2l.ai/chapter_optimization/momentum.html#the-momentum-method
        velocity = beta * velocity + gradient
        self.coef_ = self.coef_ - lr * velocity # update the coefficients
    print(self.coef_)

  def predict(self, X):
    # Funtion to run the LinearRegression on a test data point
    X = pd.DataFrame(X)
    X_test = X.copy(deep = True)
    
    if self.fit_intercept==True:
      # X = np.hstack((np.ones((X.shape[0], 1)), X))
      ones = np.ones(len(X_test)).reshape(len(X_test),1)
      X_test = np.concatenate((ones, X_test), axis=1)
    return np.dot(X_test, self.coef_)

    


  def plot_surface(self, X, y, theta_0, theta_1):
  #def plot_surface(self, X, y, T0, T1):
    '''
    Function to plot RSS (residual sum of squares) in 3D. A surface plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1 by a
    red dot. Uses self.coef_ to calculate RSS. Plot must indicate error as the title.
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to indicate RSS #pd Series of all theta_0
      :param theta_1: Value of theta_1 for which to indicate RSS #pd Series of all theta_1
      :return matplotlib figure plotting RSS
    '''
    coef0 = theta_0
    coef1 = theta_1
    theta_0 = 8
    theta_1 = 3 
    x = np.linspace(-1,1,50)
    y = theta_0 + theta_1*x
    print(theta_0, theta_1)
    def cost_func(theta_0, theta_1):
      theta_0 = np.atleast_3d(np.asarray(theta_0))
      theta_1 = np.atleast_3d(np.asarray(theta_1))
      return np.average((y -  (theta_0 + theta_1*x))**2)
    
    theta0_vals = np.linspace(self.coef_[0]-10, self.coef_[0]+10, 100)
    theta1_vals = np.linspace(self.coef_[1]-4, self.coef_[1]+4, 100)
    # theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)
    # theta0_grid = np.linspace(min(self.coef_[0]-1,0),theta_0+1,101)
    # theta1_grid = np.linspace(min(theta_1-2,0),theta_1+2,101)
    b,m = np.meshgrid(theta0_vals, theta1_vals)
    zs = np.array([cost_func(bp,mp) for bp,mp in zip(np.ravel(b), np.ravel(m))])
    Z = zs.reshape(m.shape)
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(b, m, Z, rstride=1, cstride=1, cmap=cm.coolwarm, alpha=0.7)
    k = cost_func(coef0, coef1)
    ax.scatter([coef0],[coef1],[k], c='r', s=25, marker='.')
    ax.set_xlabel('theta_0')
    ax.set_ylabel('theta_1')
    ax.set_zlabel('Error')
    plt.title("Error:"+str(k))
    return fig
    
   


  def plot_line_fit(self, X, y, theta_0, theta_1):
    """
    Function to plot fit of the line (y vs. X plot) based on chosen value of theta_0, theta_1. Plot must
    indicate theta_0 and theta_1 as the title.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting line fit
    """
    # Compute the predicted values of y based on the given theta_0 and theta_1
    y_pred = theta_0 + theta_1*X

    # Create a figure and axis object
    fig, ax = plt.subplots()

    # Plot the actual y values and predicted y values as a line
    ax.plot(X, y, 'o', label='actual')
    ax.plot(X, y_pred, 'r-', label='predicted')

    # Set the title and legend
    ax.set_title(f'Line Fit (theta_0={theta_0:.2f}, theta_1={theta_1:.2f})')
    ax.legend()

    # Return the figure object
    plt.show()
    return fig


  def plot_contour(self, X, y, theta_0, theta_1):
    """
    Plots the RSS as a contour plot. A contour plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1, and the
    direction of gradient steps. Uses self.coef_ to calculate RSS.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting the contour
    """
    # Define a grid of theta_0 and theta_1 values to evaluate the RSS on
    theta_0_grid, theta_1_grid = np.meshgrid(np.linspace(-10, 10, 100), np.linspace(-10, 10, 100))
    
    # Compute the RSS for each combination of theta_0 and theta_1
    RSS_grid = np.zeros(theta_0_grid.shape)
    for i in range(theta_0_grid.shape[0]):
        for j in range(theta_0_grid.shape[1]):
            RSS_grid[i, j] = np.mean((y - theta_0_grid[i, j] - theta_1_grid[i, j]*X)**2)
    
    # Create a figure and axis object
    fig, ax = plt.subplots()
    
    # Plot the contour lines for the RSS
    ax.contour(theta_0_grid, theta_1_grid, RSS_grid, levels=np.logspace(-1, 3, 10))
    
    # Plot the starting point as a red dot
    ax.plot(theta_0, theta_1, 'ro', markersize=10)
    
    # Set the title and axis labels
    ax.set_title('RSS Contour Plot')
    ax.set_xlabel('theta_0')
    ax.set_ylabel('theta_1')
    
    plt.show()
    # Return the figure object
    return fig

"""# q1_test.py"""

# -*- coding: utf-8 -*-
"""Q1_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qS8-5c3lwMfeY1LGlMIEVk-f-5K2bDZC
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# from linearRegression.linear_regression import LinearRegression
# from metrics import *

import time
np.random.seed(45)

N = 30
P = 5
X = pd.DataFrame(np.random.randn(N, P))
y = pd.Series(np.random.randn(N))
print(X.shape, y.shape)
print('---------------------------')


#Evaluating sklearn's implementation of linear regression
start = time.time()
LR = LinearRegression(fit_intercept=True)
LR.fit_sklearn_LR(X,y)
y_hat = LR.predict(X)
end = time.time()
print('Time taken for sklearn implementation of linear regression : ', end-start)
print('For sklearn LinearRegression : \n')
print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
print("---------------------------")


#Evaluating solution of linear regression using normal equations
start = time.time()
LR = LinearRegression(fit_intercept=True)
# LR.fit_sklearn_normal_equations(X,y)
LR.fit_normal_equations(X,y)
y_hat = LR.predict(X)
end = time.time()
print('Time taken for normal equations implementation of linear regression : ', end-start)
print('For linear regression using normal equations : \n')
print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
print("---------------------------")



#Evaluating solution of linear regression using SVD
start = time.time()
LR = LinearRegression(fit_intercept=True)
LR.fit_SVD(X,y)
y_hat = LR.predict(X)
end = time.time()
print('Time taken for SVD implementation of linear regression : ', end-start)
print('For linear regression using SVD : \n')
print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
print("---------------------------")

"""# q2_test.py"""

# -*- coding: utf-8 -*-
"""Q2_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R-h6mR7sy_h7Yi1dKE4xCb8nKprgkVDj
"""

# Correction for Assignment 4 : In Q2, you have to implement SGD with momentum 
# from scratch (for fit_SGD_with_momentum) and not use any sklearn function
# call(as mentioned in comments in the template). 
# Apologies for the mixup.


###############
# difference between Batch, Mini-Batch and Stochastic Gradient Descent for Linear Regression
# blog
# https://towardsdatascience.com/batch-mini-batch-and-stochastic-gradient-descent-for-linear-regression-9fe4eefa637c
###############



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# from linearRegression.linear_regression import LinearRegression
# from metrics import *
import sklearn
np.random.seed(45)
df=pd.DataFrame(columns=['learning rate','penalty_value','beta','Error','Time'])

N = 90
P = 10
# N = 10
# P = 4
X = pd.DataFrame(np.random.randn(N, P))
y = pd.Series(np.random.randn(N))
print(X.shape)

LR = LinearRegression(fit_intercept=True)
LR.fit_sklearn_LR(X,y)
y_hat = LR.predict(X)

print('For sklearn LinearRegression : \n')
print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
print("---------------------------")




#TODO :  Call the different variants of gradient descent here (as given in Q2)
batch_size = 10
penalty_value = [0.01,0.001,0.2,0.1,0.5]
num_iters = 500
lr = [0.05,0.1,0.01,0.2,0.001]
beta=[0.20,0.9,0.06,0.3,0.6]
#######################################

print('Stochastic Gradient Descent with manual gradient computation for unregularized oabjective:')
for i in range(len(lr)):
  for j in range(len(beta)):
    for k in range(len(penalty_value)):
      start=time.time()
      LR = LinearRegression(fit_intercept=True)
      LR.fit_SGD_with_momentum(X, y, penalty_type='l2',penalty_value=penalty_value[k], num_iters=num_iters,beta=beta[j], lr=lr[i])
      y_hat = LR.predict(X)
      end=time.time()
      error=rmse(y_hat,y)
      df.loc[len(df.index)] =[lr[i],penalty_value[k],beta[j],error,end-start]
#print('metrics')
#print('RMSE: ', rmse(y_hat, y))
#print('MAE: ', mae(y_hat, y))
#print('time:',end-start)
print(df)


print('----------------------------------')
lr=0.01
num_iters=500
penalty_value=0.5

print('Batch Gradient Descent with manual gradient computation for unregularized oabjective:')
start=time.time()
LR = LinearRegression(fit_intercept=True)
LR.fit_gradient_descent(X, y, batch_size=batch_size, gradient_type='manual', penalty_type=None,penalty_value=penalty_value, num_iters=num_iters, lr=lr)

y_hat = LR.predict(X)
end=time.time()
print('metrics')
print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
print('time:',end-start) 
error=rmse(y_hat,y)
print('Error:',error)
print('----------------------------------')

print('Batch Gradient Descent with jax gradient computation for unregularized oabjective:')
start=time.time()
LR = LinearRegression(fit_intercept=True)
LR.fit_gradient_descent(X, y, batch_size=batch_size, gradient_type='jax', penalty_type=None,penalty_value=penalty_value, num_iters=num_iters, lr=lr)

y_hat = LR.predict(X)
end=time.time()
print('metrics')
print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
print('time:',end-start) 
error=rmse(y_hat,y)
print('Error:',error) 
print('----------------------------------')

print('Batch Gradient Descent with manual gradient computation for ridge regularized oabjective:')
start=time.time()
LR = LinearRegression(fit_intercept=True)
LR.fit_gradient_descent(X, y, batch_size=batch_size, gradient_type='manual', penalty_type='l2',penalty_value=penalty_value, num_iters=num_iters, lr=lr)

y_hat = LR.predict(X)
end=time.time()
print('metrics')
print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
print('time:',end-start) 
error=rmse(y_hat,y)
print('Error:',error)  
print('----------------------------------')
print('Batch Gradient Descent with jax gradient computation for ridge regularized oabjective:')
start=time.time()
LR = LinearRegression(fit_intercept=True)
LR.fit_gradient_descent(X, y, batch_size=batch_size, gradient_type='jax', penalty_type='l2',penalty_value=penalty_value, num_iters=num_iters, lr=lr)

y_hat = LR.predict(X)
end=time.time()
print('metrics')
print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
print('time:',end-start) 
error=rmse(y_hat,y)
print('Error:',error) 
print('----------------------------------')

print('----------------------------------')
print('Batch Gradient Descent with jax gradient computation for lasso regularized oabjective:')
start=time.time()
LR = LinearRegression(fit_intercept=True)
LR.fit_gradient_descent(X, y, batch_size=batch_size, gradient_type='jax', penalty_type='l1',penalty_value=penalty_value, num_iters=num_iters, lr=lr)

y_hat = LR.predict(X)
end=time.time()

print('metrics')
print('RMSE: ', rmse(y_hat, y))
print('MAE: ', mae(y_hat, y))
print('time:',end-start) 
error=rmse(y_hat,y)
print('Error:',error)  
print('----------------------------------')

"""# q3_test"""

# -*- coding: utf-8 -*-
"""Q3_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dWm-EWFDEwI15WbGIT0_bUIrxolF7Z1G
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import imageio
# from linearRegression.linear_regression import LinearRegression
from os import path
import os
'''
N=60
x = np.array([i*np.pi/180 for i in range(60,300,2)])
np.random.seed(10)
y = 3*x + 8 + np.random.normal(0,3,len(x)) 


y=pd.Series(y)
LR = LinearRegression(fit_intercept=True)
LR.fit_gradient_descent_jax(pd.DataFrame(x), y, batch_size=40, num_iters=10, lr=0.01)
LR.plot_surface(pd.Series(x),y,LR.all_coef.iloc[0],LR.all_coef.iloc[1])
LR.plot_line_fit(pd.Series(x),y,LR.all_coef.iloc[0],LR.all_coef.iloc[1])
LR.plot_contour(pd.Series(x),y,LR.all_coef.iloc[0],LR.all_coef.iloc[1])'''


from pandas.core.array_algos.take import take_1d
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from os import path
import os

N=60
x = np.array([i*np.pi/180 for i in range(60,300,2)])
np.random.seed(10)
y = 3*x + 8 + np.random.normal(0,3,len(x)) 

y=pd.Series(y)
LR = LinearRegression(fit_intercept=True)
LR.fit_gradient_descent(pd.DataFrame(x), y, gradient_type='manual', penalty_type='l2', penalty_value=0.05, batch_size=40, num_iters=10, lr=0.01)

T_0, T_1 = [], []
for i in range(len(LR.all_coef)):
    T_0.append(LR.all_coef[i][0])
    T_1.append(LR.all_coef[i][1])
for i in range(10):
  fig =LR.plot_surface(pd.Series(x), y, LR.all_coef[i][0], LR.all_coef[i][1])
for i in range(10):
  fig = LR.plot_line_fit(pd.Series(x), y, LR.all_coef[i][0], LR.all_coef[i][1])
  fig.show()
for i in range(10):
  fig = LR.plot_contour(pd.Series(x), y, LR.all_coef[i][0], LR.all_coef[i][1])
  fig.show()

#fig = LR.plot_line_fit(pd.Series(x), y, LR.all_coef[5][0], LR.all_coef[5][1])
#fig.show()
# fig = LR.plot_line_fit(pd.Series(x), y, LR.all_coef[10][0], LR.all_coef[10][1])
#fig.show()
#fig = LR.plot_contour(pd.Series(x), y, LR.all_coef[0][0], LR.all_coef[0][1])
#fig.show()
#fig = LR.plot_contour(pd.Series(x), y, LR.all_coef[5][0], LR.all_coef[5][1])
#fig.show()
#fig = LR.plot_contour(pd.Series(x), y, LR.all_coef[10][0], LR.all_coef[10][1])
#fig.show()

"""# polynomial_features.py"""

# -*- coding: utf-8 -*-
"""polynomial_features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nT4R-FDbvZ9OmmkSuTFi1XRM8_NRCTys
"""

''' In this file, you will utilize two parameters degree and include_bias.
    Reference https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html
'''
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class PolynomialFeatures():
    
    def __init__(self, degree=2,include_bias=True):
        """
        Inputs:
        param degree : (int) max degree of polynomial features
        param include_bias : (boolean) specifies wheter to include bias term in returned feature array.
        """
        self.degree=degree
        self.include_bias=include_bias


    
    def transform(self, X):
        """
        Transform data to polynomial features
        Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. 
        For example, if an input sample is  np.array([a, b]), the degree-2 polynomial features with "include_bias=True" are [1, a, b, a^2, ab, b^2].
        
        Inputs:
        param X : (np.array) Dataset to be transformed
        
        Outputs:
        returns (np.array) Tranformed dataset.
        """
        if self.include_bias:
            X_poly = np.ones((X.shape[0], 1))
        else:
            X_poly = np.empty((X.shape[0], 0))
        for i in range(1, self.degree+1):
            X_poly = np.hstack((X_poly, np.power(X, i)))
        return X_poly

"""# q4_test.py"""

import numpy as np
import matplotlib.pyplot as plt
#from preprocessing.polynomial_features import PolynomialFeatures
#from linearRegression.linear_regression import LinearRegression
import pandas as pd
import os.path
from sklearn.preprocessing import StandardScaler
from os import path
#from metrics import *
np.random.seed(45)  #Setting seed for reproducibility

if not path.exists('Plots/Question4/'):
  os.makedirs('Plots/Question4/')


#TODO : Write here
#Preprocess the input using the polynomial features
#Solve the resulting linear regression problem by calling any one of the 
#algorithms you have implemented.

x = np.array([i*np.pi/180 for i in range(60,300,2)])
y = 3*x + 8 + np.random.normal(0,3,len(x))
x = x.reshape(x.shape[0], 1)


plt.scatter(x, y)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Input data')
plt.savefig('Plots/Question4/plot1.png')
plt.show()

degrees = range(1, 11)
norms = []

for degree in degrees:
    pFeatures = PolynomialFeatures(degree=degree)
    X_poly = pFeatures.transform(x)
    scaler = StandardScaler()
    X_poly_scaled = scaler.fit_transform(X_poly)
    LR = LinearRegression()
    # linreg.fit(X_poly_scaled, y)
    LR.fit_gradient_descent(X_poly_scaled, y, gradient_type='manual', penalty_type='l2',penalty_value=0.05, batch_size=20, num_iters=10, lr=0.01)
    norms.append(np.linalg.norm(LR.coef_))
    
    y_hat = LR.predict(X_poly_scaled)
    print('Degree : {} rmse = {} mae = {}'.format(degree, round(rmse(y_hat, y),3), round(mae(y_hat, y),3)))

plt.plot(degrees, norms)
plt.xlabel('Degree of polynomial')
plt.ylabel('Norm of theta')
plt.title('Norm of theta vs Degree of polynomial')
plt.savefig('Plots/Question4/plot2.png')
plt.show()

"""# q5_test"""

# -*- coding: utf-8 -*-
"""Q5_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19fLBYgrxvEcH2qfm0URlFaLuEUyLMibB
"""


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#from preprocessing.polynomial_features import PolynomialFeatures
#from linearRegression.linear_regression import LinearRegression
import os.path
from sklearn.preprocessing import StandardScaler
from os import path

if not path.exists('Plots/Question5/'):
    os.makedirs('Plots/Question5/')

# TODO : Write here

def generate_data(N):
    np.random.seed(0)
    X = np.random.rand(N, 1)
    y = X.squeeze()**2 + 0.1*np.random.randn(N)
    return X, y
    # X = np.array([i*np.pi/180 for i in np.linspace(60,300,N)])
    # y = 3*X + 8 + np.random.normal(0,3,len(X))
    # X = X.reshape(X.shape[0], 1)
    # return X, y

degrees = [1, 3, 5, 7, 9]
# N_list = [i for i in range(10, 101, 10)]
N_list = [10000, 100000, 1000000]

# for degree in degrees:
#     theta_norm_list = []
#     for N in N_list:
#         x, y = generate_data(N)
#         pFeatures = PolynomialFeatures(degree=degree)
#         X_poly = pFeatures.transform(x)
#         scaler = StandardScaler()
#         X_poly_scaled = scaler.fit_transform(X_poly)
#         LR = LinearRegression()
#         LR.fit_gradient_descent(X_poly_scaled, y, gradient_type='manual', penalty_type='l2', penalty_value=0.05, batch_size=10, num_iters=100, lr=0.01)
#         theta_norm_list.append(np.linalg.norm(LR.coef_))
    
#     plt.plot(N_list, theta_norm_list, label=f"degree={degree}")
    
# plt.xlabel("N (size of dataset)")
# plt.ylabel("Magnitude of norm of parameter vector, theta")
# plt.legend()
# plt.savefig('Plots/Question5/plot1.png')
# plt.show()


for N in N_list:
    theta_norm_list = []
    for degree in degrees:
        x, y = generate_data(N)
        pFeatures = PolynomialFeatures(degree=degree)
        X_poly = pFeatures.transform(x)
        scaler = StandardScaler()
        X_poly_scaled = scaler.fit_transform(X_poly)
        LR = LinearRegression()
        LR.fit_sklearn_LR(X_poly_scaled, y)
        theta_norm_list.append(np.linalg.norm(LR.coef_))
    
    plt.plot(degrees, theta_norm_list, label=f"N={N}")
    
plt.xlabel("Degree of polynomial")
plt.ylabel("Magnitude of norm of parameter vector, theta")
plt.legend()
plt.title("Magnitude of norm of parameter vector, theta vs Degree of polynomial")
plt.savefig('Plots/Question5/plot2.png')
plt.show()

"""# q6_input_normalize.py"""

import numpy as np

#TODO : Write here
# -*- coding: utf-8 -*-
"""Q6_input_normalize.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f7kCUuDOHfftApIrWG1t-OUn2FVdaxeV
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

n_samples = 200
split_ratio = 0.8
split_index = int(split_ratio * n_samples)
X, y = make_regression(n_samples=n_samples, n_features=5, random_state=52)

X_train, y_train = X[:split_index], y[:split_index]
X_test, y_test = X[split_index:], y[split_index:]

lr = LinearRegression()

lr.fit(X_train, y_train)

y_pred_unnorm = lr.predict(X_test)
error_unnorm = np.mean((y_test - y_pred_unnorm)**2)
error_unnorm_plot = y_test - y_pred_unnorm

scaler = StandardScaler()
X_train_norm = scaler.fit_transform(X_train)
X_test_norm = scaler.transform(X_test)

lr_norm = LinearRegression()
lr_norm.fit(X_train_norm, y_train)

y_pred_norm = lr_norm.predict(X_test_norm)
error_norm = np.mean((y_test - y_pred_norm)**2)
error_norm_plot = y_test - y_pred_norm

plt.bar(['Unnormalized', 'Normalized'], [error_unnorm, error_norm])
plt.ylabel('Mean Squared Error')
plt.show()

# plt.scatter(y_pred_unnorm, error_unnorm_plot)
# plt.title("Errors for Unnormalized Data")
# plt.xlabel("Predicted Values")
# plt.ylabel("Errors")
# plt.show()

# plt.scatter(y_pred_norm, error_norm_plot)
# plt.title("Errors for Normalized Data")
# plt.xlabel("Predicted Values")
# plt.ylabel("Errors")
# plt.show()

plt.scatter(y_pred_unnorm, error_unnorm_plot, label='Unnormalized')
plt.scatter(y_pred_norm, error_norm_plot, label='Normalized')
plt.legend()
plt.xlabel('Predicted Values')
plt.ylabel('Errors')
plt.title('Errors for Unnormalized and Normalized Data')
plt.show()

"""# q7_forecast.py"""

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv', header=0, index_col=0)

split_date = '1990-01-05'
split_index = df.index.get_loc(split_date)

# Create lagged variables
lags = 30
for i in range(1, lags+1):
    df[f't-{i}'] = df['Temp'].shift(i)

# Remove missing values
df.dropna(inplace=True)

# Split into train and test sets
# train_size = int(len(df) * 0.8)
train, test = df.iloc[:split_index], df.iloc[split_index:]

# Fit linear regression model
X_train, y_train = train.iloc[:, 1:], train['Temp']
X_test, y_test = test.iloc[:, 1:], test['Temp']
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on test set
y_pred = model.predict(X_test)

# Calculate RMSE
rmse = mean_squared_error(y_test, y_pred, squared=False)
print('Test RMSE:', rmse)

# Plot predictions vs true values
plt.plot(y_test.values, label='true')
plt.plot(y_pred, label='predicted')
plt.legend()
plt.show()

